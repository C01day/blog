---
layout: post
title: 机器学习笔记
subtitle: 或许哪天就忘了呢？
author: "C01day"
date: 2021-09-13
header_style: image # text
header_img: /img/in-post/cover/119.jpg
catalog: true
tags:
  - 笔记
---

## 线性回归的代价函数

$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2
$$

::: info
代价函数$J(\theta)$是关于$\theta_1,\theta_2,...,\theta_n$的函数

$h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n$

$x^{(i)}$表示第$i$个样本
:::

## 梯度下降

通过梯度下降，找到$J(\theta)$的局部最优解

$repeat\;until\;convergence:$

$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$

::: info
$\alpha:$学习率

注意所有的$\theta$是同时更新的
:::

## Logistic 回归

$$
\left.
\begin{array}{lr}
h_\theta(x)=g(\theta^Tx) \\
g(z)=\frac{1}{1+e^{-z}}
\end{array}
\right\}
\Rightarrow h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
![](https://i.loli.net/2021/09/13/GsRKOIfkLnrAxC6.png)

### Logistic 回归的代价函数

$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x),y)
$$

$$
Cost(h_\theta(x),y)=
\left\{
\begin{array}{lr}
-\log(h_\theta(x)) \quad y=1 \\
-\log(1-h_\theta(x)) \quad y=0
\end{array}
\right.
$$

![](https://i.loli.net/2021/09/13/KnxBR2GUAP58VIr.png)

$P(y=1|x;\theta)$表示给定$x,\theta$后$y=1$的概率

假设预测$y=1$的概率$h_\theta(x)=1$，而$y$的真实值是$1$，那选用第一个代价函数，此时$Cost$为$0$；如果$y$的真实值是$0$，那选用第二个代价函数，此时$Cost$为$\infty$。

将$Cost$合并简化：
$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log(1-h_\theta(x^{(i)}))\right]
$$
### 多元分类
![](https://i.loli.net/2021/09/13/KzeWLTQxs24rg9G.png)

对于每个$Class\;i$，将其转化为二分类问题，用上述$Logistic$回归的代价函数$J(\theta)$，结合梯度下降，训练各自的分类器$h_\theta^{(i)}(x)$，用来预测$y=i$时的概率

$$
h_\theta^{(i)}(x)=P(y=i|x;\theta) \quad (i=1,2,3\dots)
$$

对于一个新的输入$x$，找到最大的$h_\theta^{(i)}(x)$即可，此时的$i$便是预测的类。

## 正则化代价函数
正则化线性回归的代价函数：
$$
J(\theta)=\frac{1}{2m}\left[\;\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2+\underbrace{\lambda\sum_{j=1}^n\theta_j^2}_{Ragularization}\;\right]
$$

::: info
$m:$训练集样本容量

$n:$参数个数

正则化项是为了使参数$\theta$尽量地小，保证假设模型相对简单，避免过拟合
:::

正则化逻辑回归的代价函数和上述类似，加一个正则化项$\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$。

## 神经网络
![](https://i.loli.net/2021/09/14/eOYQAIiX21JlpVR.png)

$\Theta_{ij}^{(k)}:$右上角的$(k)$表示神经网络第$k$层与第$k+1$层之间的参数（或者说权重），右下角的$ij$表示$a_i^{k+1}$的$i$和$x_j$的$j$。

### 多元分类

![](https://i.loli.net/2021/09/15/8hsXFRyvDWIz25u.png)

图中是一个四分类问题，训练集$(x^{(i)},y^{(i)})$，其中$x^{(i)}$表示图片的特征，$y^{(i)}$是一个四维的00标签，我们想要让$h_\Theta(x^{(i)})\approx y^{(i)}$。

### 代价函数

![](https://i.loli.net/2021/09/15/xCQqBs1kRtZuDhg.png)

和逻辑回归的代价函数类似，逻辑回归的代价函数$J(\theta)$（经过正则化）是

$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log(1-h_\theta(x^{(i)}))\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
$$

神经网络的代价函数$J(\Theta)$是

$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}\log (h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
$$

::: info
$L:$神经网络层数

$s_l:$第$l$层的神经元个数（不包括偏置单元）

$K$代表输出层的单元数，等价于$s_L$

$h_\Theta(x)$是一个$K$维向量，$(h_\Theta(x))_i$表示输出向量的第$i$个值
:::

### 反向传播

![](https://i.loli.net/2021/09/15/t7hqs9JPWTVDedY.png)

一开始让$\Delta_{ij}^{(l)}=0$，用来计算$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$

遍历训练集的每个样本$(x^{(i)},y^{(i)})$

首先让输入层的激活函数$a^{(1)}=x^{(i)}$

运用正向传播算法

$$
z^{(l+1)}=\Theta^{(l)}a^{(l)} \\
a^{(l+1)}=g(z^{(l+1)})
$$

计算之后每层的激活函数$a^{(l)}\;(l=2,3,\dots,L)$

然后用样本的标签（或者说真值）$y^{(i)}$，计算输出值的误差项$\delta^{(L)}=a^{(L)}-y^{(i)}$

运用反向传播算法

$$
\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}.\ast \underbrace{g'(z^{(l)})}_{a^{(l)}.\ast(1-a^{(l)})}
$$

计算$\delta^{(l)}\;(l=L-1,L-2,\dots,2)$，注意没有$\delta^{(1)}$，因为不需要对输入层考虑误差项

最后让$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$，或者写成向量形式$\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$

由此计算出

$$
D_{ij}^{(l)}:=
\left\{
\begin{array}{lr}
\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)} \quad if \; j \neq 0 \\
\frac{1}{m}\Delta_{ij}^{(l)} \quad if \; j = 0
\end{array}
\right.
$$

而我们所需要的偏导$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$

得到了神经网络的代价函数关于每个参数$\Theta_{ij}^{(l)}$的偏导项后，就可以使用梯度下降或者其他高级优化算法了。

::: info
$.\ast$表示点乘，为矩阵的对应位置相乘
:::

### 梯度检测

反向传播算法可能会出现错误，因此需要用梯度检测来检验偏导，看是否近似

$$
\frac{J(\dots,\Theta_{ij}^{(l)}+\epsilon,\dots)-J(\dots,\Theta_{ij}^{(l)}-\epsilon,\dots)}{2\epsilon}\approx\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}
$$

::: info
由于计算量非常大，因此在训练分类器之前，需要关掉梯度检测
:::

### 随机初始化

如果将所有参数都初始化为0，会造成所有单元都相等的现象。为了避免这种情况，需要对参数进行随机初始化

$$
initialize \; each \; \Theta_{ij}^{(l)} to \; a \; random \; value \; in \; [-\epsilon,+\epsilon]
$$

## 训练集、验证集、测试集

先使用测试集对不同的假设模型$i$得到参数$\theta^{(i)}$

$$
\min_{\theta^{(i)}} J_{train}(\theta^{(i)})
$$

再用验证集选择出交叉验证误差最小的模型$i$

$$
\min_i J_{cv}(\theta^{(i)})
$$

然后用测试集计算泛化误差

$$
J_{test}(\theta^{(i)})
$$

## 偏差和方差

### 模型多项式次数$d$与偏差、方差

![](https://i.loli.net/2021/09/17/TsbuJwflKcOaRVe.png)

如果训练集误差$J_{train}(\theta)$高，验证集误差$J_{cv}(\theta)$也高（图中左侧红框），则是一个偏差问题$(bias)$；

如果训练集误差$J_{train}(\theta)$低，但验证集误差$J_{cv}(\theta)$高（图中右侧红框），则是一个方差问题$(variance)$。

### 正则化参数$\lambda$与偏差、方差

![](https://i.loli.net/2021/09/17/BMZExFULsgp3zrq.png)

::: info
图中，$J(\theta)$包括正则化项，而$J_{train}(\theta),J_{cv}(\theta)$不包括
:::

通过取不同的$\lambda$值，让$J(\theta)$最小

$$
\min_{\theta} J(\theta)
$$

得到对应的参数$\theta$，计算在参数$\theta$下$J_{train}(\theta),J_{cv}(\theta)$的变化

$\lambda$越小，惩罚程度越小，正则化项可以忽略，可能会出现过拟合现象（正则化是为了防止过拟合现象），即对训练集的拟合效果非常好，此时$J_{train}(\theta)$很小；$\lambda$越大，惩罚程度越大，可能连训练集都不能很好地拟合，此时$J_{train}(\theta)$很大。

同理，在$\lambda$很小时，可能会出现过拟合现象，对应高方差问题$(variance)$，因此$J_{cv}(\theta)$较大；在$\lambda$很大时，可能会出现欠拟合现象，对应高偏差问题$(bias)$，因此$J_{cv}(\theta)$也较大。而中间总会有某个$\lambda$值，此时的表现刚好合适。

::: info
过拟合对应高方差问题$(variance)$，$J_{train}(\theta)$较小，但$J_{cv}(\theta)$较大；

欠拟合对应高偏差问题$(bias)$，$J_{train}(\theta)$和$J_{cv}(\theta)$都较大。

上述图像比较简单和理想化，真实数据可能更加凌乱且有很多噪声，但趋势总归是正确的
:::

## 精确度和召回率

![](https://i.loli.net/2021/09/18/SRcXWL2v3bqr5PK.png)

$precision:$预测为1的数据中实际为1的比例
$$
\dfrac{true\;positive}{predicted\;positive}=\dfrac{true\;positive}{true\;positive+false\;positive}
$$

$recall:$实际为1的数据中真正被预测为1的比例
$$
\dfrac{true\;positive}{actual\;positive}=\dfrac{true\;positive}{true\;positive+false\;negative}
$$

### 精确度和召回率的权衡

![](https://i.loli.net/2021/09/18/VHvI1Z69fFzYl3b.png)

通过改变$threshold$，精确度和召回率会变化。具体来说，当$threshold$增大时，会有更高的精确度（预测为1的数据中实际为1的比例更大）和更低的召回率（实际为1的数据不变，但由于更加保守，预测为1的数据变少了）；当$threshold$减小时，会有更低的精确度和更高的召回率。

两者不可兼得，可以使用$\;F score\;$来评估算法（当然，也有很多其他评估方式）

$$
F \; score=2\dfrac{PR}{P+R}
$$

$P:precision \quad R:recall$

## 支持向量机

### 代价函数

支持向量机的代价函数和逻辑回归的代价函数类似

![](https://i.loli.net/2021/09/21/LkI9xZaB5SlpzOK.png)

$$
J(\theta)=\min_{\theta} C \sum_{i=1}^{m}\left[ y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)}) \right] + \frac{1}{2}\sum_{i=1}^{n}\theta_j^2
$$

其中，$cost_1,cost_0$和$h_\theta(x)$类似，前者是分段直线，后者是曲线（参考上图）

同时，去掉了参数$\frac{1}{m}$，并从原来的$A+\lambda B$形式变为了$CA+B$形式

当得到了使代价函数$J(\theta)$最小的参数$\theta$后，带入交叉验证集/测试集的数据$x$，就能得到支持向量机的输出$h_\theta(x)$

$$
h_\theta(x)=
\left\{
\begin{array}{lr}
1 \quad if \quad \theta^T x \geq 0 \\
0 \quad if \quad \theta^T x < 0
\end{array}
\right.
$$
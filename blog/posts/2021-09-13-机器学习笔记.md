---
layout: post
title: 机器学习笔记
subtitle: 或许哪天就忘了呢？
author: "C01day"
date: 2021-09-13
header_style: image # text
header_img: /img/in-post/cover/119.jpg
catalog: true
tags:
  - 笔记
---

## 线性回归的代价函数

$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2
$$

::: info
代价函数$J(\theta)$是关于$\theta_1,\theta_2,...,\theta_n$的函数

$h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n$

$x^{(i)}$表示第$i$个样本
:::

## 梯度下降

通过梯度下降，找到$J(\theta)$的局部最优解

$repeat\;until\;convergence:$

$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$

::: info
$\alpha:$学习率

注意所有的$\theta$是同时更新的
:::

## Logistic 回归

$$
\left.
\begin{array}{lr}
h_\theta(x)=g(\theta^Tx) \\
g(z)=\frac{1}{1+e^{-z}}
\end{array}
\right\}
\Rightarrow h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
![](https://i.loli.net/2021/09/13/GsRKOIfkLnrAxC6.png)

### Logistic 回归的代价函数

$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x),y)
$$

$$
Cost(h_\theta(x),y)=
\left\{
\begin{array}{lr}
-\log(h_\theta(x)) \quad y=1 \\
-\log(1-h_\theta(x)) \quad y=0
\end{array}
\right.
$$

![](https://i.loli.net/2021/09/13/KnxBR2GUAP58VIr.png)

$P(y=1|x;\theta)$表示给定$x,\theta$后$y=1$的概率

假设预测$y=1$的概率$h_\theta(x)=1$，而$y$的真实值是$1$，那选用第一个代价函数，此时$Cost$为$0$；如果$y$的真实值是$0$，那选用第二个代价函数，此时$Cost$为$\infty$。

将$Cost$合并简化：
$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log(1-h_\theta(x^{(i)}))\right]
$$
### 多元分类
![](https://i.loli.net/2021/09/13/KzeWLTQxs24rg9G.png)

对于每个$Class\;i$，将其转化为二分类问题，用上述$Logistic$回归的代价函数$J(\theta)$，结合梯度下降，训练各自的分类器$h_\theta^{(i)}(x)$，用来预测$y=i$时的概率

$$
h_\theta^{(i)}(x)=P(y=i|x;\theta) \quad (i=1,2,3\dots)
$$

对于一个新的输入$x$，找到最大的$h_\theta^{(i)}(x)$即可，此时的$i$便是预测的类。

## 正则化代价函数
正则化线性回归的代价函数：
$$
J(\theta)=\frac{1}{2m}\left[\;\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2+\underbrace{\lambda\sum_{j=1}^n\theta_j^2}_{Ragularization}\;\right]
$$

::: info
$m:$训练集样本容量

$n:$参数个数

正则化项是为了使参数$\theta$尽量地小，保证假设模型相对简单，避免过拟合
:::

正则化逻辑回归的代价函数和上述类似，加一个正则化项$\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2$。

## 神经网络
![](https://i.loli.net/2021/09/14/eOYQAIiX21JlpVR.png)

$\Theta_{ij}^{(k)}:$右上角的$(k)$表示神经网络第$k$层与第$k+1$层之间的参数（或者说权重），右下角的$ij$表示$a_i^{k+1}$的$i$和$x_j$的$j$。

### 多元分类

![](https://i.loli.net/2021/09/15/8hsXFRyvDWIz25u.png)

图中是一个四分类问题，训练集$(x^{(i)},y^{(i)})$，其中$x^{(i)}$表示图片的特征，$y^{(i)}$是一个四维的00标签，我们想要让$h_\Theta(x^{(i)})\approx y^{(i)}$。

### 代价函数

![](https://i.loli.net/2021/09/15/xCQqBs1kRtZuDhg.png)

和逻辑回归的代价函数类似，逻辑回归的代价函数$J(\theta)$（经过正则化）是

$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log(1-h_\theta(x^{(i)}))\right]+\frac{\lambda}{2m}\sum_{j=1}^n\theta_j^2
$$

神经网络的代价函数$J(\Theta)$是

$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^m\sum_{k=1}^Ky_k^{(i)}\log (h_\Theta(x^{(i)}))_k+(1-y_k^{(i)})\log(1-(h_\Theta(x^{(i)}))_k)\right]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
$$

::: info
$L:$神经网络层数

$s_l:$第$l$层的神经元个数（不包括偏置单元）

$K$代表输出层的单元数，等价于$s_L$

$h_\Theta(x)$是一个$K$维向量，$(h_\Theta(x))_i$表示输出向量的第$i$个值
:::

### 反向传播

![](https://i.loli.net/2021/09/15/t7hqs9JPWTVDedY.png)

一开始让$\Delta_{ij}^{(l)}=0$，用来计算$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$

遍历训练集的每个样本$(x^{(i)},y^{(i)})$

首先让输入层的激活函数$a^{(1)}=x^{(i)}$

运用正向传播算法

$$
z^{(l+1)}=\Theta^{(l)}a^{(l)} \\
a^{(l+1)}=g(z^{(l+1)})
$$

计算之后每层的激活函数$a^{(l)}\;(l=2,3,\dots,L)$

然后用样本的标签（或者说真值）$y^{(i)}$，计算输出值的误差项$\delta^{(L)}=a^{(L)}-y^{(i)}$

运用反向传播算法

$$
\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}.\ast \underbrace{g'(z^{(l)})}_{a^{(l)}.\ast(1-a^{(l)})}
$$

计算$\delta^{(l)}\;(l=L-1,L-2,\dots,2)$，注意没有$\delta^{(1)}$，因为不需要对输入层考虑误差项

最后让$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_j^{(l)}\delta_i^{(l+1)}$，或者写成向量形式$\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}(a^{(l)})^T$

由此计算出

$$
D_{ij}^{(l)}:=
\left\{
\begin{array}{lr}
\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)} \quad if \; j \neq 0 \\
\frac{1}{m}\Delta_{ij}^{(l)} \quad if \; j = 0
\end{array}
\right.
$$

而我们所需要的偏导$\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=D_{ij}^{(l)}$

得到了神经网络的代价函数关于每个参数$\Theta_{ij}^{(l)}$的偏导项后，就可以使用梯度下降或者其他高级优化算法了。

::: info
$.\ast$表示点乘，为矩阵的对应位置相乘
:::
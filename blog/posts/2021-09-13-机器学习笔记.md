---
layout: post
title: 机器学习笔记
subtitle: 或许哪天就忘了呢？
author: "C01day"
date: 2021-09-13
header_style: image # text
header_img: /img/in-post/cover/119.jpg
catalog: true
tags:
  - 笔记
---

## 线性回归的代价函数

$$
J(\theta)=\frac{1}{2m}\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2
$$

::: info
代价函数$J(\theta)$是关于$\theta_1,\theta_2,...,\theta_n$的函数

$h_\theta(x)=\theta^Tx=\theta_0x_0+\theta_1x_1+\cdots+\theta_nx_n$

$x^{(i)}$表示第$i$个样本
:::

## 梯度下降

通过梯度下降，找到$J(\theta)$的局部最优解

$repeat\;until\;convergence:$

$$
\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta)
$$

::: info
$\alpha:$学习率

注意所有的$\theta$是同时更新的
:::

## Logistic 回归

$$
\left.
\begin{array}{lr}
h_\theta(x)=g(\theta^Tx) \\
g(z)=\frac{1}{1+e^{-z}}
\end{array}
\right\}
\Rightarrow h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$
![](https://i.loli.net/2021/09/13/GsRKOIfkLnrAxC6.png)

### Logistic 回归的代价函数

$$
J(\theta)=\frac{1}{m}\sum_{i=1}^{m}Cost(h_\theta(x),y)
$$

$$
Cost(h_\theta(x),y)=
\left\{
\begin{array}{lr}
-\log(h_\theta(x)) \quad y=1 \\
-\log(1-h_\theta(x)) \quad y=0
\end{array}
\right.
$$

![](https://i.loli.net/2021/09/13/KnxBR2GUAP58VIr.png)

$P(y=1|x;\theta)$表示给定$x,\theta$后$y=1$的概率

假设预测$y=1$的概率$h_\theta(x)=1$，而$y$的真实值是$1$，那选用第一个代价函数，此时$Cost$为$0$；如果$y$的真实值是$0$，那选用第二个代价函数，此时$Cost$为$\infty$。

将$Cost$合并简化：
$$
J(\theta)=-\frac{1}{m}\left[\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log(1-h_\theta(x^{(i)}))\right]
$$
### 多元分类
![](https://i.loli.net/2021/09/13/KzeWLTQxs24rg9G.png)

对于每个$Class\;i$，将其转化为二分类问题，用上述$Logistic$回归的代价函数$J(\theta)$，结合梯度下降，训练各自的分类器$h_\theta^{(i)}(x)$，用来预测$y=i$时的概率

$$
h_\theta^{(i)}(x)=P(y=i|x;\theta) \quad (i=1,2,3\dots)
$$

对于一个新的输入$x$，找到最大的$h_\theta^{(i)}(x)$即可，此时的$i$便是预测的类。

## 正则化

$$
J(\theta)=\frac{1}{2m}\left[\;\sum_{i=1}^m\left(h_\theta(x^{(i)})-y^{(i)}\right)^2+\lambda\sum_{j=1}^n\theta_j^2\;\right]
$$

::: info
$m:$训练集样本容量

$n:$参数个数
:::
